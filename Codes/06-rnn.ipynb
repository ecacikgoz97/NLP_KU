{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Language Modeling using LSTM on Penn treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Iterators, IterTools, Knet, Printf, LinearAlgebra, StatsBase, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charset\n",
    "This will hold our set of characters that the model will be able to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Charset"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Charset\n",
    "    c2i::Dict{Any,Int}\n",
    "    i2c::Vector{Any}\n",
    "    eow::Int\n",
    "    mask::Int\n",
    "end\n",
    "\n",
    "function Charset(charset::String; eow=\"\", mask=\"-\")\n",
    "    i2c = [ eow; mask; [ c for c in charset ]  ]\n",
    "    c2i = Dict( c => i for (i, c) in enumerate(i2c))\n",
    "    return Charset(c2i, i2c, c2i[eow], c2i[mask])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextReader\n",
    "Here we will read our input files and split into characters line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TextReader\n",
    "    file::String\n",
    "    charset::Charset\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    s === nothing && (s = open(r.file))\n",
    "    eof(s) && return close(s)\n",
    "    return [ get(r.charset.c2i, c, r.charset.eow) for c in readline(s)], s\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataIterator\n",
    "In this iterator we will handle iterating over the data, and prepare our models input and output by converting lines of similar lengths into mini training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arraybatch (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct DataIterator\n",
    "    src::TextReader\n",
    "    batchsize::Int\n",
    "    maxlength::Int\n",
    "    batchmajor::Bool\n",
    "    bucketwidth::Int\n",
    "    buckets::Vector\n",
    "    batchmaker::Function\n",
    "end\n",
    "\n",
    "function DataIterator(src::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 2, numbuckets = min(128, maxlength รท bucketwidth))\n",
    "    # buckets[i] is an array of sentence pairs with similar length\n",
    "    buckets = [ [] for i in 1:numbuckets ]\n",
    "    DataIterator(src, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{DataIterator}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{DataIterator}) = Base.HasEltype()\n",
    "Base.eltype(::Type{DataIterator}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::DataIterator, state=nothing)\n",
    "    # When file is finished but buckets are partially full\n",
    "    if state == 0\n",
    "        for i in 1:length(d.buckets)\n",
    "            if length(d.buckets[i]) > 0\n",
    "                batch = d.batchmaker(d, d.buckets[i])\n",
    "                d.buckets[i] = []\n",
    "                return batch, state\n",
    "            end\n",
    "        end\n",
    "        # terminate iteration\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    while true\n",
    "        src_next = iterate(d.src, state)\n",
    "\n",
    "        if src_next === nothing\n",
    "            state = 0\n",
    "            return iterate(d, state)\n",
    "        end\n",
    "\n",
    "        (src_word, src_state) = src_next\n",
    "        state = src_state\n",
    "        src_length = length(src_word)\n",
    "\n",
    "        (src_length > d.maxlength) && continue\n",
    "        (src_length < 2) && continue\n",
    "\n",
    "        i = Int(ceil(src_length / d.bucketwidth))\n",
    "        i > length(d.buckets) && (i = length(d.buckets))\n",
    "\n",
    "        push!(d.buckets[i], src_word)\n",
    "        if length(d.buckets[i]) == d.batchsize\n",
    "            batch = d.batchmaker(d, d.buckets[i])\n",
    "            d.buckets[i] = []\n",
    "            return batch, state\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function arraybatch(d::DataIterator, bucket)\n",
    "    src_eow = d.src.charset.eow\n",
    "    src_lengths = map(x -> length(x), bucket)\n",
    "    max_length = max(src_lengths...)\n",
    "    x = zeros(Int64, length(bucket), max_length + 2)\n",
    "\n",
    "    for (i, v) in enumerate(bucket)\n",
    "        to_be_added = fill(src_eow, max_length - length(v) + 1)\n",
    "        x[i,:] = [src_eow; v; to_be_added]\n",
    "    end\n",
    "\n",
    "    # default d.batchmajor is false\n",
    "    d.batchmajor && (x = x')\n",
    "\n",
    "    # the output in lang. model is same as input sequence but shifted one step\n",
    "    return (x[:, 1:end-1], x[:, 2:end])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmul(w,x) = (w == 1 ? x : w == 0 ? 0 : reshape(w * reshape(x,size(x,1),:), (:, size(x)[2:end]...)))\n",
    "\n",
    "struct Embed; w; end\n",
    "Embed(charsetsize::Int, embedsize::Int) = Embed(param(embedsize, charsetsize))\n",
    "(l::Embed)(x) = l.w[:, x]\n",
    "\n",
    "struct Linear; w; b; end\n",
    "Linear(inputsize::Int, outputsize::Int) = Linear(param(outputsize, inputsize), param0(outputsize))\n",
    "(l::Linear)(x) = mmul(l.w,x) .+ l.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLL Loss masking function\n",
    "This function masks the training output array by zeros, this way we don't propagate loss from paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mask(a, pad)\n",
    "    a = copy(a)\n",
    "    for i in 1:size(a, 1)\n",
    "        j = size(a,2)\n",
    "        while a[i, j] == pad && j > 1\n",
    "            if a[i, j - 1] == pad\n",
    "                a[i, j] = 0\n",
    "            end\n",
    "            j -= 1\n",
    "        end\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language model\n",
    "Our model consists of four layers. Size of their outputs are as the following, where T is sequence length, B is batchsize, H is the hidden size of RNN, and V is vocabulary size, E is the embedding size:\n",
    "\n",
    "* **(T, B)** - Input\n",
    "* **(E, T, B)** - Embedding\n",
    "* **(H, T, B)** - RNN\n",
    "* **(V, T, B)** - Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct LModel\n",
    "    srcembed::Embed\n",
    "    rnn::RNN\n",
    "    projection::Linear\n",
    "    dropout::Real\n",
    "    srccharset::Charset\n",
    "end\n",
    "\n",
    "function LModel(hidden::Int, srcembsz::Int, srccharset::Charset; layers=1, dropout=0)\n",
    "\n",
    "    srcembed = Embed(length(srccharset.i2c), srcembsz)\n",
    "    rnn = RNN(srcembsz, hidden; bidirectional=false, numLayers=layers, dropout=dropout)\n",
    "    projection = Linear(hidden, length(srccharset.i2c))\n",
    "\n",
    "    LModel(srcembed, rnn, projection, dropout, srccharset)\n",
    "end\n",
    "\n",
    "function (s::LModel)(src, tgt; average=true)\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    srcembed = s.srcembed(src)\n",
    "    rnn_out = s.rnn(srcembed)\n",
    "    dims = size(rnn_out)\n",
    "    output = s.projection(dropout(reshape(rnn_out, dims[1], dims[2] * dims[3]), s.dropout))\n",
    "    scores = reshape(output, size(output, 1), dims[2], dims[3])\n",
    "    nll(scores, mask(tgt, s.srccharset.eow); dims=1, average=average)\n",
    "end\n",
    "\n",
    "function generate(s::LModel; start=\"\", maxlength=30)\n",
    "    s.rnn.h, s.rnn.c = 0, 0\n",
    "    chars = fill(s.srccharset.eow, 1)\n",
    "    start = [ c for c in start ]\n",
    "    starting_index = 1\n",
    "    for i in 1:length(start)\n",
    "        push!(chars, s.srccharset.c2i[start[i]])\n",
    "        charembed = s.srcembed(chars[i:i])\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        starting_index += 1\n",
    "    end\n",
    "\n",
    "    for i in starting_index:maxlength\n",
    "        charembed = s.srcembed(chars[i:i])\n",
    "        rnn_out = s.rnn(charembed)\n",
    "        output = s.projection(dropout(rnn_out, s.dropout))\n",
    "        push!(chars, s.srccharset.c2i[ sample(s.srccharset.i2c, Weights(Array(softmax(reshape(output, length(s.srccharset.i2c)))))) ] )\n",
    "\n",
    "        if chars[end] == s.srccharset.eow\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    join([ s.srccharset.i2c[i] for i in chars ], \"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "Here we define a `loss(model, data)` which returns a `(ฮฃloss, Nloss)` pair if `average=false` and\n",
    "a `ฮฃloss/Nloss` average if `average=true` for a whole dataset.\n",
    "\n",
    "`report_lm(loss)` calculates character perplexity and bit-per-character metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report_lm (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, data; average=true)\n",
    "    mean([model(x,y) for (x,y) in data])\n",
    "end\n",
    "\n",
    "report_lm(loss) = (loss=loss, ppl=exp.(loss), bpc=loss ./ log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "Train our model using Adam optimizer and saving the best performing model on dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(model, steps, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn), steps=steps) do y\n",
    "        devloss = loss(model, dev)\n",
    "        tstloss = map(d->loss(model,d), tst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (trn=report_lm(tstloss), dev=report_lm(devloss))\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "โ Info: Reading data\n",
      "โ @ Main In[11]:9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataIterator(TextReader(\"/Users/emrecanacikgoz/Desktop/Comp442/data/ptb/test.txt\", Charset(Dict{Any, Int64}('w' => 48, '7' => 19, 'o' => 40, '5' => 17, '<' => 22, 'h' => 33, 'i' => 34, 'r' => 43, 'q' => 42, 'a' => 26โฆ), Any[\"\", \"-\", ' ', '#', '$', '&', '\\'', '*', '-', '.'  โฆ  'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'], 1, 2)), 16, 256, false, 2, Vector{Any}[[], [], [], [], [], [], [], [], [], []  โฆ  [], [], [], [], [], [], [], [], [], []], arraybatch)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed random generators for reproducability\n",
    "Random.seed!(123);\n",
    "\n",
    "# Define char set, batchsize and max sequence length\n",
    "char_set = \" #\\$&'*-./0123456789<>N\\\\abcdefghijklmnopqrstuvwxyz\"\n",
    "datadir = \"/Users/emrecanacikgoz/Desktop/Comp442/data/ptb\"\n",
    "BATCHSIZE, MAXLENGTH = 16, 256\n",
    "\n",
    "@info \"Reading data\"\n",
    "charset = Charset(char_set)\n",
    "train_reader = TextReader(\"$datadir/train.txt\", charset)\n",
    "dev_reader = TextReader(\"$datadir/valid.txt\", charset)\n",
    "test_reader = TextReader(\"$datadir/test.txt\", charset)\n",
    "\n",
    "dtrn = DataIterator(train_reader, batchsize=BATCHSIZE, maxlength=MAXLENGTH)\n",
    "ddev = DataIterator(dev_reader, batchsize=BATCHSIZE, maxlength=MAXLENGTH)\n",
    "dtst = DataIterator(test_reader, batchsize=BATCHSIZE, maxlength=MAXLENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "โ Info: Initializing Language Model\n",
      "โ @ Main In[12]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LModel(Embed(P(Matrix{Float32}(256,51))), LSTM(input=256,hidden=256,layers=2,dropout=0.2), Linear(P(Matrix{Float32}(51,256)), P(Vector{Float32}(51))), 0.2, Charset(Dict{Any, Int64}('w' => 48, '7' => 19, 'o' => 40, '5' => 17, '<' => 22, 'h' => 33, 'i' => 34, 'r' => 43, 'q' => 42, 'a' => 26โฆ), Any[\"\", \"-\", ' ', '#', '$', '&', '\\'', '*', '-', '.'  โฆ  'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'], 1, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Initializing Language Model\"\n",
    "epochs = 10\n",
    "ctrn = collect(dtrn)\n",
    "trn = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "trnmini = ctrn[1:20]\n",
    "dev = collect(ddev)\n",
    "model = LModel(256, 256, charset; layers=2, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "โ Info: Starting training ...\n",
      "โ @ Main In[13]:1\n",
      "\n",
      "\r",
      "โฃ                    โซ [0.00%, 1/26340, 01:02/453:22:47, 61.97s/i] (trn = (loss = (3.7943077f0,), ppl = (44.447453f0,), bpc = (5.4740289150063015,)), dev = (loss = 3.770252f0, ppl = 43.391f0, bpc = 5.439323847958092))"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] differentiate(::Function; o::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "   @ AutoGrad ~/.julia/packages/AutoGrad/1QZxP/src/core.jl:165",
      " [2] differentiate",
      "   @ ~/.julia/packages/AutoGrad/1QZxP/src/core.jl:140 [inlined]",
      " [3] iterate",
      "   @ ~/.julia/packages/Knet/YIFWC/src/train20/train.jl:26 [inlined]",
      " [4] iterate(p::Knet.Train20.Progress{Knet.Train20.Minimize{Vector{Tuple{Matrix{Int64}, Matrix{Int64}}}}}, s::Int64)",
      "   @ Knet.Train20 ~/.julia/packages/Knet/YIFWC/src/train20/progress.jl:73",
      " [5] progress!(::Function, ::Vararg{Any}; o::Base.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:steps,), Tuple{Int64}}})",
      "   @ Knet.Train20 ~/.julia/packages/Knet/YIFWC/src/train20/progress.jl:60",
      " [6] train!(model::LModel, steps::Int64, trn::Vector{Tuple{Matrix{Int64}, Matrix{Int64}}}, dev::Vector{Tuple{T, T} where T}, tst::Vector{Tuple{T, T} where T})",
      "   @ Main ./In[9]:3",
      " [7] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "@info \"Starting training ...\"\n",
    "model = train!(model, length(ctrn), trn, dev, trnmini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Finished training, Starting evaluation ...\"\n",
    "trnloss = loss(model, dtrn);\n",
    "println(\"Training set scores:       \", report_lm(trnloss))\n",
    "devloss = loss(model, ddev);\n",
    "println(\"Development set scores:    \", report_lm(devloss))\n",
    "tstloss = loss(model, dtst);\n",
    "println(\"Test set scores:           \", report_lm(tstloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling sentences from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Sample sentences from the model\"\n",
    "print(\"enter prompt or leave empty for no prompt, CTRL+C to exit\")\n",
    "\n",
    "while true\n",
    "    print(\"prompt:\")\n",
    "    prompt = lowercase(readline())\n",
    "    println(generate(model; start=prompt, maxlength=1024))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
