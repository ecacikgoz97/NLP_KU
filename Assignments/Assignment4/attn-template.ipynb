{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Machine Translation\n",
    "\n",
    "**Reference:** Luong, Thang, Hieu Pham and Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412-1421. 2015.\n",
    "\n",
    "* https://www.aclweb.org/anthology/D15-1166/ (main paper reference)\n",
    "* https://arxiv.org/abs/1508.04025 (alternative paper url)\n",
    "* https://github.com/tensorflow/nmt (main code reference)\n",
    "* https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention (alternative code reference)\n",
    "* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py:2449,2103 (attention implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet, Test, Base.Iterators, Printf, LinearAlgebra, Random, CUDA, IterTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and data from previous projects\n",
    "\n",
    "Please copy or include the following types and related functions from previous projects:\n",
    "`Vocab`, `TextReader`, `MTData`, `Embed`, `Linear`, `mask!`, `loss`, `int2str`,\n",
    "`bleu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Vocab\n",
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    cnt = Dict{String,Int}()\n",
    "    for line in eachline(file)\n",
    "        for word in tokenizer(line)\n",
    "            cnt[word] = 1 + get(cnt, word, 0)\n",
    "        end\n",
    "    end\n",
    "    delete!(cnt, unk); delete!(cnt, eos)\n",
    "    words = sort!(collect(keys(cnt)); by=(w->cnt[w]), rev=true)\n",
    "    if mincount > 1; i=findfirst(w -> cnt[w]<mincount, words); i!==nothing && (words=words[1:i-1]); end\n",
    "    if unk != nothing; pushfirst!(words, unk); end\n",
    "    if eos != nothing; pushfirst!(words, eos); end\n",
    "    if length(words) > vocabsize; words = words[1:vocabsize]; end\n",
    "    vocab = Dict(words[i] => i for i in 1:length(words))\n",
    "    Vocab(vocab, words, get(vocab,unk,0), get(vocab,eos,0), tokenizer)\n",
    "end\n",
    "\n",
    "# Text Reader\n",
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    if s==nothing; s=open(r.file, \"r\"); end\n",
    "    if eof(s); return nothing; end\n",
    "    w2i_list = [get(r.vocab.w2i, word, r.vocab.unk) for word in r.vocab.tokenizer(readline(s))]\n",
    "    return w2i_list, s\n",
    "end\n",
    "\n",
    "# MT Data\n",
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(src::TextReader, tgt::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 10, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar source sentence length\n",
    "    MTData(src, tgt, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}\n",
    "\n",
    "function Base.iterate(d::MTData, state=nothing)\n",
    "    # Your code here\n",
    "    source, target = d.src, d.tgt\n",
    "    if state === nothing; for b in d.buckets; empty!(b); end; end\n",
    "    \n",
    "    bucket = nothing; ibucket = nothing;\n",
    "    while true\n",
    "        iterate_source = (state===nothing ? iterate(source) : iterate(source, state[1]))\n",
    "        iterate_target = (state===nothing ? iterate(target) : iterate(target, state[2]))\n",
    "        if iterate_source === nothing || iterate_target === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            if ibucket === nothing; bucket = nothing; else; bucket = d.buckets[ibucket]; end\n",
    "            break\n",
    "        else\n",
    "            sent_source, state_source = iterate_source; sent_target, state_target = iterate_target;\n",
    "            state = (state_source, state_target); sents = (sent_source, sent_target); source_length = length(sents[1]);\n",
    "            (source_length == 0) && continue;(source_length > d.maxlength) && continue\n",
    "            ibucket = min(1 + Int(ceil(source_length / d.bucketwidth)), length(d.buckets)); bucket = d.buckets[ibucket]; \n",
    "            push!(bucket, sents)\n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    batch = d.batchmaker(d, bucket)\n",
    "    empty!(bucket)\n",
    "    return (batch, state)\n",
    "end\n",
    "\n",
    "function arraybatch(d::MTData, bucket)\n",
    "    \n",
    "    batch_size = length(bucket)\n",
    "    eos = d.src.vocab.eos\n",
    "    \n",
    "    source = [data[1] for data in bucket]; target = [data[2] for data in bucket]\n",
    "    source_len = maximum(length.(source)); target_len = maximum(length.(target))\n",
    "    \n",
    "    sbatched = fill(eos, batch_size, source_len); tbatched = fill(eos, batch_size, target_len+2)\n",
    "    for idx in 1:batch_size\n",
    "        sbatched[idx, 1+end-length(source[idx]):end] = source[idx]\n",
    "        tbatched[idx, 1] = eos; tbatched[idx, 2:1+length(target[idx])] = target[idx]; tbatched[idx, 2+length(target[idx])] = eos\n",
    "    end\n",
    "    if d.batchmajor == true; sbatched, tbatched = sbatched', tbatched'; end\n",
    "    return sbatched, tbatched\n",
    "    \n",
    "end\n",
    "\n",
    "# Embedding Layer\n",
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    # Your code here\n",
    "    Embed(param(embedsize, vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    # Your code here\n",
    "    l.w[:,x]\n",
    "end\n",
    "\n",
    "# Linear Layer\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    Linear(param(outputsize, inputsize), param0(outputsize))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    l.w * x .+ l.b\n",
    "end\n",
    "\n",
    "# Masking\n",
    "function mask!(a,pad)\n",
    "    # Your code here\n",
    "    matrix = copy(a)\n",
    "    for row in 1:size(matrix, 1)\n",
    "        col = size(matrix, 2)\n",
    "        while col > 1 && matrix[row, col] == pad \n",
    "            if matrix[row, col - 1] == pad; matrix[row, col] = 0; end\n",
    "            col -= 1\n",
    "        end\n",
    "    end\n",
    "    return matrix\n",
    "end\n",
    "\n",
    "function loss(model, data; average=true)\n",
    "    # Your code here\n",
    "    losses=0; nums=0;\n",
    "    for (x,y) in data\n",
    "        (loss, num) = model(x, y; average=false)\n",
    "        losses += loss\n",
    "        nums += num\n",
    "    end\n",
    "\n",
    "    if average\n",
    "        return losses / nums\n",
    "    else\n",
    "        return losses, nums\n",
    "    end\n",
    "end\n",
    "\n",
    "# Utility to convert int arrays to sentence strings\n",
    "function int2str(y,vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w->!isequal(w,vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1+length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end\n",
    "\n",
    "# BLUE\n",
    "function bleu(s2s,d::MTData)\n",
    "    d = MTData(d.src,d.tgt,batchsize=1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile,hyp = mktemp()\n",
    "    for (x,y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i in 1:size(y,1)\n",
    "            println(hyp, int2str(g[i,:], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\", \"multi-bleu.perl\")\n",
    "    run(pipeline(`cat $hypfile`,`perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2S: Sequence to sequence model with attention\n",
    "\n",
    "In this project we will define, train and evaluate a sequence to sequence encoder-decoder\n",
    "model with attention for Turkish-English machine translation. The model has two extra\n",
    "fields compared to `S2S_v1`: the `memory` layer computes keys and values from the encoder,\n",
    "the `attention` layer computes the attention vector for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Memory; w; end\n",
    "\n",
    "struct Attention; wquery; wattn; scale; end\n",
    "\n",
    "struct S2S\n",
    "    srcembed::Embed       # encinput(B,Tx) -> srcembed(Ex,B,Tx)\n",
    "    encoder::RNN          # srcembed(Ex,B,Tx) -> enccell(Dx*H,B,Tx)\n",
    "    memory::Memory        # enccell(Dx*H,B,Tx) -> keys(H,Tx,B), vals(Dx*H,Tx,B)\n",
    "    tgtembed::Embed       # decinput(B,Ty) -> tgtembed(Ey,B,Ty)\n",
    "    decoder::RNN          # tgtembed(Ey,B,Ty) . attnvec(H,B,Ty)[t-1] = (Ey+H,B,Ty) -> deccell(H,B,Ty)\n",
    "    attention::Attention  # deccell(H,B,Ty), keys(H,Tx,B), vals(Dx*H,Tx,B) -> attnvec(H,B,Ty)\n",
    "    projection::Linear    # attnvec(H,B,Ty) -> proj(Vy,B,Ty)\n",
    "    dropout::Real         # dropout probability\n",
    "    srcvocab::Vocab       # source language vocabulary\n",
    "    tgtvocab::Vocab       # target language vocabulary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and data\n",
    "\n",
    "We will load a pretrained model (16.20 bleu) for code testing.  The data should be loaded\n",
    "with the vocabulary from the pretrained model for word id consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if !isdefined(Main, :pretrained) || pretrained === nothing\n",
    "    @info \"Loading reference model\"\n",
    "    if !isfile(\"comp542attn.jld2\")\n",
    "        download(\"https://github.com/denizyuret/Knet.jl/releases/download/v1.4.5/comp542attn.tar.gz\", \"comp542attn.tar.gz\")\n",
    "        run(`tar xzf comp542attn.tar.gz`)\n",
    "    end\n",
    "    pretrained = Knet.load(\"comp542attn.jld2\", \"model\")\n",
    "end\n",
    "datadir = \"datasets/tr_to_en\"\n",
    "if !isdir(datadir)\n",
    "    @info \"Downloading data\"\n",
    "    download(\"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\", \"qi18naacl-dataset.tar.gz\")\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    BATCHSIZE, MAXLENGTH = 64, 50\n",
    "    @info \"Reading data\"\n",
    "    tr_vocab = pretrained.srcvocab # Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = pretrained.tgtvocab # Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    dtrn = MTData(tr_train, en_train, batchsize=BATCHSIZE, maxlength=MAXLENGTH)\n",
    "    ddev = MTData(tr_dev, en_dev, batchsize=BATCHSIZE)\n",
    "    dtst = MTData(tr_test, en_test, batchsize=BATCHSIZE)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Model constructor\n",
    "\n",
    "The `S2S` constructor takes the following arguments:\n",
    "* `hidden`: size of the hidden vectors for both the encoder and the decoder\n",
    "* `srcembsz`, `tgtembsz`: size of the source/target language embedding vectors\n",
    "* `srcvocab`, `tgtvocab`: the source/target language vocabulary\n",
    "* `layers=1`: number of layers\n",
    "* `bidirectional=false`: whether the encoder is bidirectional\n",
    "* `dropout=0`: dropout probability\n",
    "\n",
    "Hints:\n",
    "* You can find the vocabulary size with `length(vocab.i2w)`.\n",
    "* If the encoder is bidirectional `layers` must be even and the encoder should have `layers÷2` layers.\n",
    "* The decoder will use \"input feeding\", i.e. it will concatenate its previous output to its input. Therefore the input size for the decoder should be `tgtembsz+hidden`.\n",
    "* Only `numLayers`, `dropout`, and `bidirectional` keyword arguments should be used for RNNs, leave everything else default.\n",
    "* The memory parameter `w` is used to convert encoder states to keys. If the encoder is bidirectional initialize it to a `(hidden,2*hidden)` parameter, otherwise set it to the constant 1.\n",
    "* The attention parameter `wquery` is used to transform the query, set it to the constant 1 for this project.\n",
    "* The attention parameter `scale` is used to scale the attention scores before softmax, set it to a parameter of size 1.\n",
    "* The attention parameter `wattn` is used to transform the concatenation of the decoder output and the context vector to the attention vector. It should be a parameter of size `(hidden,2*hidden)` if unidirectional, `(hidden,3*hidden)` if bidirectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function S2S(hidden::Int, srcembsz::Int, tgtembsz::Int, srcvocab::Vocab, tgtvocab::Vocab;\n",
    "             layers=1, bidirectional=false, dropout=0)\n",
    "    # Your code here\n",
    "    \n",
    "    srcembed = Embed(length(srcvocab.i2w), srcembsz)\n",
    "    \n",
    "    if bidirectional\n",
    "        encoder = RNN(srcembsz, hidden, bidirectional=bidirectional, numLayers=layers/2, dropout=dropout)\n",
    "        w = param(hidden, 2*hidden); wattn = param(hidden, 3*hidden);\n",
    "    else\n",
    "        encoder = RNN(srcembsz, hidden, bidirectional=bidirectional, numLayers=layers, dropout=dropout)\n",
    "        w = 1; wattn = param(hidden, 2*hidden);\n",
    "    end\n",
    "    \n",
    "    tgtembed   = Embed(length(tgtvocab.i2w), tgtembsz)\n",
    "    decoder    = RNN(tgtembsz+hidden, hidden, bidirectional=false, numLayers=layers, dropout=dropout)\n",
    "    projection = Linear(hidden, length(tgtvocab.i2w))\n",
    "    \n",
    "    wquery = 1; scale = param0(1);\n",
    "    \n",
    "    memory = Memory(w); attention = Attention(wquery, wattn, scale);\n",
    "    \n",
    "    return S2S(srcembed, encoder, memory, tgtembed, decoder, attention, projection, dropout, srcvocab, tgtvocab)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Testing S2S constructor\" begin\n",
    "    H,Ex,Ey,Vx,Vy,L,Dx,Pdrop = 8,9,10,length(dtrn.src.vocab.i2w),length(dtrn.tgt.vocab.i2w),2,2,0.2\n",
    "    m = S2S(H,Ex,Ey,dtrn.src.vocab,dtrn.tgt.vocab;layers=L,bidirectional=(Dx==2),dropout=Pdrop)\n",
    "    @test size(m.srcembed.w) == (Ex,Vx)\n",
    "    @test size(m.tgtembed.w) == (Ey,Vy)\n",
    "    @test m.encoder.inputSize == Ex\n",
    "    @test m.decoder.inputSize == Ey + H\n",
    "    @test m.encoder.hiddenSize == m.decoder.hiddenSize == H\n",
    "    @test m.encoder.direction == Dx-1\n",
    "    @test m.encoder.numLayers == (Dx == 2 ? L÷2 : L)\n",
    "    @test m.decoder.numLayers == L\n",
    "    @test m.encoder.dropout == m.decoder.dropout == Pdrop\n",
    "    @test size(m.projection.w) == (Vy,H)\n",
    "    @test size(m.memory.w) == (Dx == 2 ? (H,2H) : ())\n",
    "    @test m.attention.wquery == 1\n",
    "    @test size(m.attention.wattn) == (Dx == 2 ? (H,3H) : (H,2H))\n",
    "    @test size(m.attention.scale) == (1,)\n",
    "    @test m.srcvocab === dtrn.src.vocab\n",
    "    @test m.tgtvocab === dtrn.tgt.vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Memory\n",
    "\n",
    "The memory layer turns the output of the encoder to a pair of tensors that will be used as\n",
    "keys and values for the attention mechanism. Remember that the encoder RNN output has size\n",
    "`(H*D,B,Tx)` where `H` is the hidden size, `D` is 1 for unidirectional, 2 for\n",
    "bidirectional, `B` is the batchsize, and `Tx` is the sequence length. It will be\n",
    "convenient to store these values in batch major form for the attention mechanism, so\n",
    "*values* in memory will be a permuted copy of the encoder output with size `(H*D,Tx,B)`\n",
    "(see `@doc permutedims`). The *keys* in the memory need to have the same first dimension\n",
    "as the *queries* (i.e. the decoder hidden states). So *values* will be transformed into\n",
    "*keys* of size `(H,B,Tx)` with `keys = m.w * values` where `m::Memory` is the memory\n",
    "layer. Note that you will have to do some reshaping to 2-D and back to 3-D for matrix\n",
    "multiplications. Also note that `m.w` may be a scalar such as `1` e.g. when `D=1` and we\n",
    "want keys and values to be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (m::Memory)(x)\n",
    "    # Your code here\n",
    "    v = x; #(H*D,B,Tx)\n",
    "    \n",
    "    v = permutedims(v, (1,3,2)) #(H*D,Tx,B)\n",
    "    k = mmul(m.w, v)\n",
    "    \n",
    "    return (k, v)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following helper function for scaling and linear transformations of 3-D tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmul(w,x) = (w == 1 ? x : w == 0 ? 0 : reshape(w * reshape(x,size(x,1),:), (:, size(x)[2:end]...)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Testing memory\" begin\n",
    "    H,D,B,Tx = pretrained.encoder.hiddenSize, pretrained.encoder.direction+1, 4, 5\n",
    "    x = KnetArray(randn(Float32,H*D,B,Tx))\n",
    "    k,v = pretrained.memory(x)\n",
    "    @test v == permutedims(x,(1,3,2))\n",
    "    @test k == mmul(pretrained.memory.w, v)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Encoder\n",
    "\n",
    "`encode()` takes a model `s` and a source language minibatch `src`. It passes the input\n",
    "through `s.srcembed` and `s.encoder` layers with the `s.encoder` RNN hidden states\n",
    "initialized to `0` in the beginning, and copied to the `s.decoder` RNN at the end. The\n",
    "steps so far are identical to `S2S_v1` but there is an extra step: The encoder output is\n",
    "passed to the `s.memory` layer which returns a `(keys,values)` pair. `encode()` returns\n",
    "this pair to be used later by the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function encode(s::S2S, src)\n",
    "    # Your code here\n",
    "    s.encoder.h, s.encoder.c = 0, 0\n",
    "    \n",
    "    srcembed        = s.srcembed(src)\n",
    "    rnn_encoder_out = s.encoder(srcembed)\n",
    "    \n",
    "    s.decoder.h, s.decoder.c = s.encoder.h, s.encoder.c\n",
    "    \n",
    "    keys,values = s.memory(rnn_encoder_out)\n",
    "    \n",
    "    return (keys,values)\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Testing encoder\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,D,B,Tx = pretrained.encoder.hiddenSize, pretrained.encoder.direction+1, size(src1,1), size(src1,2)\n",
    "    @test size(key1) == (H,Tx,B)\n",
    "    @test size(val1) == (H*D,Tx,B)\n",
    "    @test (pretrained.decoder.h,pretrained.decoder.c) === (pretrained.encoder.h,pretrained.encoder.c)\n",
    "    @test norm(key1) ≈ 1214.4755f0\n",
    "    @test norm(val1) ≈ 191.10411f0\n",
    "    @test norm(pretrained.decoder.h) ≈ 48.536964f0\n",
    "    @test norm(pretrained.decoder.c) ≈ 391.69028f0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Attention\n",
    "\n",
    "The attention layer takes `cell`: the decoder output, and `mem`: a pair of (keys,vals)\n",
    "from the encoder, and computes and returns the attention vector. First `a.wquery` is used\n",
    "to linearly transform the cell to the query tensor. The query tensor is reshaped and/or\n",
    "permuted as appropriate and multiplied with the keys tensor to compute the attention\n",
    "scores. Please see `@doc bmm` for the batched matrix multiply operation used for this\n",
    "step. The attention scores are scaled using `a.scale` and normalized along the time\n",
    "dimension using `softmax`. After the appropriate reshape and/or permutation, the scores\n",
    "are multiplied with the `vals` tensor (using `bmm` again) to compute the context\n",
    "tensor. After the appropriate reshape and/or permutation the context vector is\n",
    "concatenated with the cell and linearly transformed to the attention vector using\n",
    "`a.wattn`. Please see the paper and code examples for details.\n",
    "\n",
    "Note: the paper mentions a final `tanh` transform, however the final version of the\n",
    "reference code does not use `tanh` and gets better results. Therefore we will skip `tanh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (a::Attention)(cell, mem)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Testing attention\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,B = pretrained.encoder.hiddenSize, size(src1,1)\n",
    "    x = KnetArray(randn(Float32,H,B,5))\n",
    "    y = pretrained.attention(x, (key1, val1))\n",
    "    @test size(y) == size(x)\n",
    "    @test isapprox(norm(y), 810.0; rtol=0.05)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Decoder\n",
    "\n",
    "`decode()` takes a model `s`, a target language minibatch `tgt`, the memory from the\n",
    "encoder `mem` and the decoder output from the previous time step `prev`. After the input\n",
    "is passed through the embedding layer, it is concatenated with `prev` (this is called\n",
    "input feeding). The resulting tensor is passed through `s.decoder`. Finally the\n",
    "`s.attention` layer takes the decoder output and the encoder memory to compute the\n",
    "\"attention vector\" which is returned by `decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function decode(s::S2S, tgt, mem, prev)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Testing decoder\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    key1,val1 = encode(pretrained, src1)\n",
    "    H,B = pretrained.encoder.hiddenSize, size(src1,1)\n",
    "    cell = randn!(similar(key1, size(key1,1), size(key1,3), 1))\n",
    "    cell = decode(pretrained, tgt1[:,1:1], (key1,val1), cell)\n",
    "    @test size(cell) == (H,B,1)\n",
    "    @test isapprox(norm(cell), 132.0; rtol=0.05)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Loss\n",
    "\n",
    "The loss function takes source language minibatch `src`, and a target language minibatch\n",
    "`tgt` and returns `sumloss/numwords` if `average=true` or `(sumloss,numwords)` if\n",
    "`average=false` where `sumloss` is the total negative log likelihood loss and `numwords` is\n",
    "the number of words predicted (including a final eos for each sentence). The source is first\n",
    "encoded using `encode` yielding a `(keys,vals)` pair (memory). Then the decoder is called to\n",
    "predict each word of `tgt` given the previous word, `(keys,vals)` pair, and the previous\n",
    "decoder output. The previous decoder output is initialized with zeros for the first\n",
    "step. The output of the decoder at each step is passed through the projection layer giving\n",
    "word scores. Losses can be computed from word scores and masked/shifted `tgt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S)(src, tgt; average=true)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Testing loss\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    @test pretrained(src1,tgt1) ≈ 1.4666592f0\n",
    "    sumloss,cntloss = pretrained(src1,tgt1,average=false)\n",
    "    @test sumloss ≈ 1949.1901f0 && cntloss == 1329\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Greedy translator\n",
    "\n",
    "An `S2S` object can be called with a single argument (source language minibatch `src`, with\n",
    "size `B,Tx`) to generate translations (target language minibatch with size `B,Ty`). The\n",
    "keyword argument `stopfactor` determines how much longer the output can be compared to the\n",
    "input. Similar to the loss function, the source minibatch is encoded yield a `(keys,vals)`\n",
    "pair (memory). We generate the output one time step at a time by calling the decoder with\n",
    "the last output, the memory, and the last decoder state. The last output is initialized to\n",
    "an array of `eos` tokens and the last decoder state is initialized to an array of\n",
    "zeros. After computing the scores for the next word using the projection layer, the highest\n",
    "scoring words are selected and appended to the output. The generation stops when all outputs\n",
    "in the batch have generated `eos` or when the length of the output is `stopfactor` times the\n",
    "input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S)(src; stopfactor = 3)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Testing translator\" begin\n",
    "    src1,tgt1 = first(dtrn)\n",
    "    tgt2 = pretrained(src1)\n",
    "    @test size(tgt2) == (64, 41)\n",
    "    @test tgt2[1:3,1:3] == [14 25 10647; 37 25 1426; 27 5 349]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8. Training\n",
    "\n",
    "`trainmodel` creates, trains and returns an `S2S` model. The arguments are described in\n",
    "comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainmodel(trn,                  # Training data\n",
    "                    dev,                  # Validation data, used to determine the best model\n",
    "                    tst...;               # Zero or more test datasets, their loss will be periodically reported\n",
    "                    bidirectional = true, # Whether to use a bidirectional encoder\n",
    "                    layers = 2,           # Number of layers (use `layers÷2` for a bidirectional encoder)\n",
    "                    hidden = 512,         # Size of the hidden vectors\n",
    "                    srcembed = 512,       # Size of the source language embedding vectors\n",
    "                    tgtembed = 512,       # Size of the target language embedding vectors\n",
    "                    dropout = 0.2,        # Dropout probability\n",
    "                    epochs = 0,           # Number of epochs (one of epochs or iters should be nonzero for training)\n",
    "                    iters = 0,            # Number of iterations (one of epochs or iters should be nonzero for training)\n",
    "                    bleu = false,         # Whether to calculate the BLEU score for the final model\n",
    "                    save = false,         # Whether to save the final model\n",
    "                    seconds = 60,         # Frequency of progress reporting\n",
    "                    )\n",
    "    @show bidirectional, layers, hidden, srcembed, tgtembed, dropout, epochs, iters, bleu, save; flush(stdout)\n",
    "    model = S2S(hidden, srcembed, tgtembed, trn.src.vocab, trn.tgt.vocab;\n",
    "                layers=layers, dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    epochs == iters == 0 && return model\n",
    "\n",
    "    (ctrn,cdev,ctst) = collect(trn),collect(dev),collect.(tst)\n",
    "    traindata = (epochs > 0\n",
    "                 ? collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "                 : shuffle!(collect(take(cycle(ctrn), iters))))\n",
    "\n",
    "    bestloss, bestmodel = loss(model, cdev), deepcopy(model)\n",
    "    progress!(adam(model, traindata), seconds=seconds) do y\n",
    "        devloss = loss(model, cdev)\n",
    "        tstloss = map(d->loss(model,d), ctst)\n",
    "        if devloss < bestloss\n",
    "            bestloss, bestmodel = devloss, deepcopy(model)\n",
    "        end\n",
    "        println(stderr)\n",
    "        (dev=devloss, tst=tstloss, mem=Float32(CUDA.usage[]))\n",
    "    end\n",
    "    save && Knet.save(\"attn-$(Int(time_ns())).jld2\", \"model\", bestmodel)\n",
    "    bleu && Main.bleu(bestmodel,dev)\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model: If your implementation is correct, the first epoch should take about 24\n",
    "minutes on a v100 and bring the loss from 9.83 to under 4.0. 10 epochs would take about 4\n",
    "hours on a v100. With other GPUs you may have to use a smaller batch size (if memory is\n",
    "lower) and longer time (if gpu speed is lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the appropriate option for training:\n",
    "model = pretrained  # Use reference model\n",
    "# model = Knet.load(\"attn-1538395466294882.jld2\", \"model\")  # Load pretrained model\n",
    "# model = trainmodel(dtrn,ddev,take(dtrn,20); epochs=10, save=true, bleu=true)  # Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to sample translations from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = MTData(tr_dev, en_dev, batchsize=1) |> collect;\n",
    "function translate_sample(model, data)\n",
    "    (src,tgt) = rand(data)\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "    println(\"REF: \", int2str(tgt,model.tgtvocab))\n",
    "    println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations for random instances from the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sample(model, data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to generate translations from user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function translate_input(model)\n",
    "    v = model.srcvocab\n",
    "    src = [ get(v.w2i, w, v.unk) for w in v.tokenizer(readline()) ]'\n",
    "    out = model(src)\n",
    "    println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "    println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate translations for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate_input(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition\n",
    "\n",
    "The reference model `pretrained` has 16.2 bleu. By playing with the optimization algorithm\n",
    "and hyperparameters, using per-sentence loss, and (most importantly) splitting the Turkish\n",
    "words I was able to push the performance to 21.0 bleu. I will give extra credit to groups\n",
    "that can exceed 21.0 bleu in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
